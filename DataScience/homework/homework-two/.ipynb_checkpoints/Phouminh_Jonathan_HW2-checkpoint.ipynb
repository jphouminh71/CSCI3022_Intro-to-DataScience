{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='top'></a>\n",
    "\n",
    "# Homework 2: Data Visualization and Probability Analysis \n",
    "***\n",
    "\n",
    "**Name**: Jonathan Phouminh\n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **MIDNIGHT on Monday September 14**. Your solutions to theoretical questions should be done in Markdown directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your classmates, but **you must write all code and solutions on your own**.\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Any relevant data sets should be available under the **Data** module on Canvas. To make life easier on the graders if they need to run your code, do not change the relative path names here. Instead, move the files around on your computer.\n",
    "- If you're not familiar with typesetting math directly into Markdown then by all means, do your work on paper first and then typeset it later.  Remember that there is a [reference guide](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference) linked on Canvas on writing math in Markdown. **All** of your written commentary, justifications and mathematical work should be in Markdown.\n",
    "- Because you can technically evaluate notebook cells is a non-linear order, it's a good idea to do Kernel $\\rightarrow$ Restart & Run All as a check before submitting your solutions.  That way if we need to run your code you will know that it will work as expected. \n",
    "- It is **bad form** to make your reader interpret numerical output from your code.  If a question asks you to compute some value from the data you should show your code output **AND** write a summary of the results in Markdown directly below your code. \n",
    "- 95 points of this assignment are in problems.  The remaining 5 are for neatness, style, and overall exposition of both code and text.\n",
    "- This probably goes without saying, but... For any question that asks you to calculate something, you **must show all work and justify your answers to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit. \n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Problem 3](#p3) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborators:\n",
    "\n",
    "   - Zachary Chomalla\n",
    "   - Bao Nguyen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id='p1'></a>\n",
    "\n",
    "## (15 points) Problem 1: Theory (Median Distance)\n",
    "***\n",
    "\n",
    "One way we conceptualize many data science questions is asking for the \"best choice\" of some parameter on data set.  We should be able to justify that our measures of centrality should in some way be the \"best\" ways to represent the data.\n",
    "\n",
    "\n",
    "In lecture, we may have discussed the following important property of the mean:\n",
    "\n",
    "\n",
    "The *sample mean* of data $X_1, X_2, \\dots X_n$ is the unique minimizer $c$ of the function $$f(c)=\\sum_{i=1}^n \\left(X_i-c \\right)^2. $$\n",
    "\n",
    "The proof of that claim is as follows:\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Differentiating yields\n",
    "$$f'(c)=\\frac{df}{dc}\\sum_{i=1}^n \\left(X_i-c \\right)^2 =\\sum_{i=1}^n-2(X_i-c).$$ \n",
    "\n",
    "Setting $f'(c)=0$ gives\n",
    "\n",
    "$$0=\\sum_{i=1}^n-2(X_i-c)$$\n",
    "$$=2nc-2\\sum_{i=1}^n X_i$$\n",
    "$$\\implies\\qquad  c=\\frac{\\sum_{i=1}^n X_i}{n}=\\bar{X}$$\n",
    "\n",
    "***\n",
    "\n",
    "### Your exercise:\n",
    "\n",
    "You are tasked with recreating a *similar* proof.  Prove the following:\n",
    "\n",
    "The *median* of data $X_1, X_2, \\dots X_n$ is the possibly non-unique minimizer $c$ of the function $$f(c)=\\sum_{i=1}^n |X_i-c| $$\n",
    "\n",
    "A few things to think about:\n",
    "\n",
    " - how do we differentiate the absolute value function?\n",
    " - what conditions might make the median non-unique in this case?  If it's nonunique, what possible values of $c$ still minimize the function $f$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Want to show that there exist some minimizer $c$ such that applying f(c) gives us the median** \n",
    "\n",
    "**We will see that the value of C turns out to be non-unique**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "\n",
    "Differentiating yields\n",
    "$$f'(c)=\\frac{df}{dc}\\sum_{i=1}^n |X_i-c|  =\\sum_{i=1}^n - \\frac{(x_i - c)}{|x_i - c|}.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this equation we notice that this is just an alternating series for values {1, -1} depending on what the value of C is equal to. So we can look at this equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i=1}^n Z_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where Z is equal to -1 or 1 depending on $x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \\begin{cases} \n",
    "      -1  & c \\leq x_i\\\\\n",
    "      1 & 100 > x \\\\\n",
    " \\end{cases}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this knowledge, given some data set lets say [1,2,3,4,5,6]. We should pick a C such that $Z_i$ is minimized (close to zero). So we need to pick a C value in this data set such that $len(x_i \\leq C)$ = $len(x_i > C )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the given data set we would end up choosing some value in between the values (3,4). Any value in between that range suffices and will result in a valid value for $c$ such that f($c$) is the median of the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was exactly what we were looking for and we have shown that f($c$) is an equivalent process to find the median of a data set. Lastly, with the example data set we can conclude that $c$ is non-unique because for data sets with an even number of elements we were able to choose an infinite amount of values for C that were in between the two inner most values, and this applies to all data sets $f$ where the amount of values are even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "<a id='p2'></a>\n",
    "\n",
    "## (40 pts) Problem 2: Computation (Streaming Means)\n",
    "***\n",
    "\n",
    "Data science is often divided into two categories: questions of *what* the best value might be to repreesnt a data problem, and questions of *how* to compute that data value.  Question 1 - and prior lectures - should tell you that computing the mean is valuable!  But *how* do we compute the mean?\n",
    "\n",
    "Let $x_1, x_2, \\ldots, x_n$ be $n$ observations of a variable of interest.  Recall that the sample mean $\\bar{x}_n$ and sample variance $s^2_n$ are given by \n",
    "<a id='eq1'></a>\n",
    "$$\n",
    "\\bar{x}_n = \\frac{1}{n}\\sum_{k=1}^n x_k \\quad \\textrm{and} \\quad s^2_n = \\frac{1}{n-1}\\sum_{k=1}^n \\left( x_k - \\bar{x}_n\\right)^2 \\qquad \\tag{Equation 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**:\n",
    "\n",
    "How many computations - floating point operations: addition, subtraction, multiplication, division each count as 1 operation - are required to compute the mean of the data set with $n$ observations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explantion / Thought Process**\n",
    "\n",
    "Base case, we would have n = 1 and would require at most 1 computation, dividing the observation n by 1. \n",
    "\n",
    "For sets that include at least 2 values, when we examine the observation the computation required to sum up the values we find that it takes n-1 operations of addition. Then we account for the dividing operation at the end and add 1. \n",
    "\n",
    "Therefore, the required amount of computations for a data set with $n$ observations is at most,  $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**:\n",
    "\n",
    "Now suppose our data is *streaming*- we slowly add observations one at a time, instead of seeing the entire data set at once.  We are still interested in the mean, so if we stream the data set `[4,6,0,10, ...]`, we first compute the mean of the the first data point `[4]`, then we recompute the mean of the first two points `[4,6]`, then we recompute the mean of three `[4,6,0]`, and so forth.\n",
    "\n",
    "Suppose we recompute the mean from scratch after each and every one of our $n$ observations are one-by-one added to our data set.  How many floating point operations are spent computing (and re-computing) the mean of the data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation / Thought Process**\n",
    "\n",
    "For the $n'th$ value added to the data set, we would add $n$ amount of operations\n",
    "\n",
    "\n",
    "$$\n",
    "    OpCost(n) = \\sum_{k=1}^n k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be convinced that streaming a mean costs a lot more computer time than just computing once!\n",
    "\n",
    "In this problem we explore a smarter method for such an _online_ computation of the mean.  \n",
    "\n",
    "**Result**: The following relation holds between the mean of the first $n-1$ observations and the mean of all $n$ observations: \n",
    "\n",
    "$$\n",
    "\\bar{x}_n = \\bar{x}_{n-1} + \\frac{x_n - \\bar{x}_{n-1}}{n}\n",
    "$$\n",
    "\n",
    "\n",
    "A proof of this result is in the [Appendix](#Appendix) after problem 3, and requires some careful manipulations of the sum $\\bar{x}_n$.  Your task will be to computationally verify and utilize this result.\n",
    "\n",
    "**Part C**: Write a function `my_sample_mean` that takes as its input a numpy array and returns the mean of that numpy array using the formulas from class ([Equation 1](#eq1)). Write another function `my_sample_var` that takes as its input a numpy array and returns the variance of that numpy array, again using the formulas from class ([Equation 1](#eq1)). You may **not** use any built-in sample mean or variance functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_sample_mean(myArray):\n",
    "    sum = 0\n",
    "    for x in myArray:\n",
    "        sum = sum + x\n",
    "    return sum / len(myArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Use your functions from Part B to compute the sample mean and sample variance of the following array, which contains the minutes late that the BuffBus is running on Friday afternoon.\n",
    "\n",
    "`bus = [312, 4, 10, 0, 22, 39, 81, 19, 8, 60, 80, 42]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  56.4166666667\n",
      "Variance of Bus:  7274.628787878787\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def findVariance(givenArray):\n",
    "    mean = my_sample_mean(givenArray)    \n",
    "    mathprocess = lambda x: math.pow((x-mean),2)\n",
    "    updatedArray = map(mathprocess,givenArray)\n",
    "    summation = sum(updatedArray)\n",
    "    variance = (1/(len(givenArray)-1)) * summation\n",
    "    return (variance, mean)\n",
    "\n",
    "bus = np.array([312, 4, 10, 0, 22, 39, 81, 19, 8, 60, 80, 42])\n",
    "solutions = findVariance(bus)\n",
    "print(\"Mean: \", solutions[1])\n",
    "print(\"Variance of Bus: \", solutions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Implement a third function called `update_mean` that implements the formula discussed after part B. Note that this function will need to take as its input three things: $x_n$, $\\bar{x}_{n-1}$ and $n$, and returns $\\bar{x}_{n}$. A function header and return statement are provided for you. This function may be auto-graded, so please do not change the given header API - the order of inputs matters! If you change it, you might lose points.\n",
    "\n",
    "Use this function to compute the values that you get from taking the mean of the first buff buses' lateness, the first two buff buses' lateness, the first three buff buses' lateness, and so on up to all of the `bus` data points from **Part D**. Store your streaming bus means in a numpy array called `buffbus_bad_means`.  Report all 12 estimates in `buffbus_bad_means`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of Means:  [312.0, 158.0, 108.66666666666666, 81.5, 69.599999999999994, 64.5, 66.857142857142861, 60.875, 55.0, 55.5, 57.727272727272727, 56.416666666666664]\n"
     ]
    }
   ],
   "source": [
    "# Given API:\n",
    "def update_mean(prev_mean, xn, n):\n",
    "    y = (xn - prev_mean) / n \n",
    "    now_mean = y + prev_mean\n",
    "    return now_mean\n",
    "\n",
    "#Your code here (to loop over the full data)\n",
    "def bad_mean(bus):\n",
    "    generatedMeans = []\n",
    "    prevMean = 0\n",
    "    currentSizeN = 1\n",
    "    for value in bus: \n",
    "        prevMean = update_mean(prevMean, value, currentSizeN)\n",
    "        generatedMeans.append(prevMean)\n",
    "        currentSizeN = currentSizeN + 1\n",
    "        \n",
    "    print(\"Array of Means: \", generatedMeans)\n",
    "        \n",
    "bus = np.array([312, 4, 10, 0, 22, 39, 81, 19, 8, 60, 80, 42])\n",
    "bad_mean(bus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may report any results for part E here, if not done using print() statements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure your function complies with the given API, run this small test, where we suppose we have a mean of $\\bar{x}_n = 1$ with the first $2$ data points (`prev_mean`), and we update this with the 3rd ($n=3$) data point which is $x_3=2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert update_mean(1,2,3)==4/3, \"Warning: function seems broken.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**:\n",
    "\n",
    "How many floating point operations were spent computing the final result in your code in **part E**?  Is this truly better than the uninformed approach from **part B**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    TotalOps(n) = \\sum_{k=1}^n k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#top)\n",
    "\n",
    "<a id='p3'></a>\n",
    "\n",
    "## (40 pts) Problem 3: Data (Probability and Histograms)\n",
    "*** \n",
    "The sinking of the RMS Titanic was a terrible tragedy that saw the loss of many lives. Even within this tragedy, thanks to the combinations of the records of the White Star Line and the thorough nature of follow-up research after the accident we have some records that can help us try to piece together the course of events on board the ship. Many of the historians and other researchers who have investigated this event have speculated as to what exactly happened.\n",
    "\n",
    "We have the data on survival rates by class, gender, and age, so let's figure out whether there is evidence for some of these scenarios. Access the Titanic data in `titanic_data.csv` and store it in a Pandas DataFrame. The data contains information pertaining to class status (**Pclass**), survival (**Survived**), and gender (**Sex**) of passengers, among other things. Be sure to use the `titanic_data.csv` data set, *not* the `clean_titanic_data` file or `dirty_titanic_data` file from the in-class notebook exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'titanic_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-83fe0ba757a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'titanic_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanphouminh/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanphouminh/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanphouminh/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanphouminh/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathanphouminh/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'titanic_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "filepath = 'titanic_data.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**:\n",
    "Based on the overall population of passengers, report the probability of survival.\n",
    "\n",
    "$$P(Survived=1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survivors = df[\"Survived\"].sum()\n",
    "totalPassengers = len(df.index)\n",
    "print(\"P(SURVIVED = 1): \", survivors / totalPassengers) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: \n",
    "Some claim that the final hours aboard the RMS Titanic were marked by \"class warfare\" in which the people with first-class tickets took all the good spots on the lifeboats; others claim that the final hours were characterized by male chivalry, in which the men valiantly gave up their positions in the boats and succumbed bravely to the depths of the Atlantic. \n",
    "\n",
    "Consider the two claims: class warfare, and male chivalry. Suppose that class warfare occurred in the final hours aboard the Titanic.  What patterns might you expect to see in the data?  Suppose that male chivalry was widespread during the final hours instead. What patterns might you then expect to see in the data?  Explain both of these hypothesized patterns in words. Are these two hypotheses mutually exclusive or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Warfare**\n",
    "    We should expect to see that a lot more survivors who have a low level pclass along with a high fare value.\n",
    "    \n",
    "**Chivalry**\n",
    "    We should expect to see that a lot more men would not have survived if if this was the case, regardless of their pclass or fare\n",
    "    \n",
    "These two events are not mutually exclusive because it wouldn't make sense for a man to show chivalry because it would be a contradiction if they got on the boats because of their class at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Use Pandas methods to create a clean data set by removing any rows from the DataFrame that are missing values corresponding to **Survived**, **Pclass**, **Age**, or **Sex**. Store the clean data in a DataFrame called dfTitanic. Be sure to show any exploratory work determining if/where there are rows with missing values. _HINT: There should be 714 rows in your cleaned data set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''titanicDf = df.loc[(df[\"Survived\"].isnull() == False)\n",
    "                    & (df[\"Pclass\"].isnull() == False)\n",
    "                    & (df[\"Age\"].isnull() == False)\n",
    "                    & (df[\"Sex\"].isnull() == False)\n",
    "                  ]\n",
    "'''\n",
    "dfTitanic = df.dropna(subset=['Survived', 'Pclass', 'Age', 'Sex'])\n",
    "len(dfTitanic.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Compute the probability of survival according to class, gender, and all combinations of the two variables.  Then, answer the following questions:\n",
    "* **(i)** When reviewing class survival probability, how do the results compare to the base survival probability results from **Part A**?\n",
    "* **(ii)** When reviewing gender survival probability, how do the results compare to the base survival probability results from **Part A**?\n",
    "* **(iii)** Within each passenger class, were men or women more/less/equally likely to survive?\n",
    "* **(iv)**  Did men in first class or women in third class have a higher survival probability?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "survivors = df[\"Survived\"].sum()\n",
    "totalPassengers = len(df.index)\n",
    "print(\"P(SURVIVED = 1): \", survivors / totalPassengers) \n",
    "'''\n",
    "'''\n",
    "def ComputeProbability(A, B):\n",
    "    # if A and B are the same  =>  {M,M} , {F,F}\n",
    "    if A == B:\n",
    "        listOfA = titanicDf.loc[titanicDf[\"Sex\"] == A]\n",
    "        listOfASurvivors = listOfA.loc[listOfA[\"Survived\"] == 1]\n",
    "        return len(listOfASurvivors.index)\n",
    "\n",
    "    else:\n",
    "        # want all the rows that both A and B are true for the column\n",
    "        return\n",
    "'''\n",
    "\n",
    "# just compute all of this manually for now\n",
    "\n",
    "totalPassengers = len(dfTitanic.index)\n",
    "\n",
    "# Probabilities for computing based on Class \n",
    "print()\n",
    "print(\"PROBABILITY OF SURVIVORS BASED ON PCLASS\")\n",
    "\n",
    "listOfp1 =  dfTitanic.loc[dfTitanic[\"Pclass\"] == 1]\n",
    "listOfp1Survivors = listOfp1.loc[listOfp1[\"Survived\"] == 1]\n",
    "print(\"P(Pclass = 1): \", listOfp1Survivors[\"Survived\"].sum() / totalPassengers)\n",
    "\n",
    "listOfp2 =  dfTitanic.loc[dfTitanic[\"Pclass\"] == 2]\n",
    "listOfp2Survivors = listOfp2.loc[listOfp2[\"Survived\"] == 1]\n",
    "print(\"P(Pclass = 2): \", listOfp2Survivors[\"Survived\"].sum() / totalPassengers)\n",
    "\n",
    "listOfp3 =  dfTitanic.loc[dfTitanic[\"Pclass\"] == 3]\n",
    "listOfp3Survivors = listOfp3.loc[listOfp3[\"Survived\"] == 1]\n",
    "print(\"P(Pclass = 3): \", listOfp3Survivors[\"Survived\"].sum() / totalPassengers)\n",
    "print(\"==========================================\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Probabilities of computing based on Gender\n",
    "print(\"PROBABILITY OF SURVIVORS BASED ON GENDER\")\n",
    "\n",
    "listOfMales = dfTitanic.loc[dfTitanic[\"Sex\"] == \"male\"]\n",
    "listOfMaleSurvivors = listOfMales.loc[listOfMales[\"Survived\"] == 1]\n",
    "print(\"P(Gender=Male): \", listOfMaleSurvivors[\"Survived\"].sum() / totalPassengers)\n",
    "\n",
    "listOffemales = dfTitanic.loc[dfTitanic[\"Sex\"] == \"female\"]\n",
    "listOfFemaleSurvivors = listOffemales.loc[listOffemales[\"Survived\"] == 1]\n",
    "print(\"P(Gender=Female): \", listOfFemaleSurvivors[\"Survived\"].sum() / totalPassengers)\n",
    "print(\"==========================================\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Remaining Combinations of the two variables\n",
    "print(\"PROBABILITY OF SURVIVORS BASED ON GENDER / PCLASS\")\n",
    "\n",
    "C1Males = listOfMales.loc[listOfMales[\"Pclass\"] == 1]\n",
    "C1MSurvivors = C1Males.loc[C1Males[\"Survived\"] == 1]\n",
    "print(\"P(Male & C1): \", C1MSurvivors[\"Survived\"].sum() / totalPassengers)\n",
    "\n",
    "C2Males = listOfMales.loc[listOfMales[\"Pclass\"] == 2]\n",
    "C2MSurvivors = C2Males.loc[C2Males[\"Survived\"] == 1]\n",
    "print(\"P(Male & C2): \", C2MSurvivors[\"Survived\"].sum() / totalPassengers)\n",
    "\n",
    "C3Males = listOfMales.loc[listOfMales[\"Pclass\"] == 3]\n",
    "C3MSurvivors = C3Males.loc[C3Males[\"Survived\"] == 1]\n",
    "print(\"P(Male & C3): \", C3MSurvivors[\"Survived\"].sum() / totalPassengers)\n",
    "\n",
    "C1Females = listOffemales.loc[listOffemales[\"Pclass\"] == 1]\n",
    "C1FSurvivors = C1Females.loc[C1Females[\"Survived\"] == 1]\n",
    "print(\"P(Female & C1): \", C1FSurvivors[\"Survived\"].sum() / totalPassengers)\n",
    "\n",
    "C2Females = listOffemales.loc[listOffemales[\"Pclass\"] == 2]\n",
    "C2FSurvivors = C2Females.loc[C2Females[\"Survived\"] == 1]\n",
    "print(\"P(Female & C2): \", C2FSurvivors[\"Survived\"].sum() / totalPassengers)\n",
    "\n",
    "C3Females = listOffemales.loc[listOffemales[\"Pclass\"] == 3]\n",
    "C3FSurvivors = C3Females.loc[C3Females[\"Survived\"] == 1]\n",
    "print(\"P(Female & C3): \", C3FSurvivors[\"Survived\"].sum() / totalPassengers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Responses**\n",
    "\n",
    "(i) The results of surivability for each class is less than the base survivability. And we expect this because we are breaking down the total probability of survivors based on each different passenger class and each class should be lower than the base survival probability. \n",
    "      Class one passengers were the most likely to survive, followed by class 3, followed by class 2. \n",
    "     \n",
    "(ii) We see from the gender probability describes the base survivablity but more specifically separates men from women. And we see that women were more likely to survive than men based on the data. \n",
    "\n",
    "(iii) \n",
    "  - In passenger class 1, women were more likely to survive than men\n",
    "  - In passenger class 2, males were more likely to survive than women \n",
    "  - In passenger class 3, women were more likely to survive than men\n",
    "  \n",
    "(iv) Women were still more likely to survive then the men in first class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: One might wonder how a passenger's age is related to the likelihood that they would survive the Titanic disaster. In addition to the \"male chivalry\" argument outlined above, you can perhaps imagine an addendum - \"women and children first!\" - as the cry to ring out across the decks. Or you might imagine the opposite - rather than \"class warfare\", it is simply healthy adults fighting to take lifeboat spots for themselves.\n",
    "\n",
    "To answer this question graphically, plot two density histograms on the same set of axes, showing the distribution of the ages of passengers who survived, and the distribution of the ages of passengers who did not. \n",
    "* Use the bin edges $[0,5,10,\\ldots,70,75,80]$ for both histograms.\n",
    "* To better distinguish between our populations, we will represent survivors with `navy` (as they were eventually rescued by ships) and those who passed away with `sandybrown`.\n",
    "* Plot both histograms on a single set of axes (there should be only one panel in the figure you create), but use Matplotlib/Pandas plotting functionality to make the faces of the histogram boxes somewhat transparent, so both histograms are visible.\n",
    "* Include a legend and label your axes.\n",
    "* Comment on the results. Does your figure suggest that some age ranges are more or less likely to have survived the disaster than other ages? Fully explain your reasoning and use your figure to justify your conclusions.\n",
    "* If you noticed some relationship between age and likelihood of survival, what is one possible explanation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bins = range(0,80,5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "dfTitanic.loc[dfTitanic[\"Survived\"]==1].hist(column=\"Age\", ax=ax,alpha = 0.75,facecolor=\"navy\", density = True, bins = my_bins)\n",
    "dfTitanic.loc[dfTitanic[\"Survived\"]==0].hist(column=\"Age\", ax=ax,alpha = 0.3, facecolor=\"sandybrown\", density = True, bins = my_bins)\n",
    "\n",
    "ax.set_title(\"Survival / Death Among Ages\", fontsize = 15)\n",
    "ax.set_ylabel(\"Density\", fontsize = 13)\n",
    "ax.set_xlabel(\"Age\", fontsize = 13)\n",
    "ax.grid(alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F:** In Part E, we plotted two *density* histograms, showing the distributions of ages of passengers that survived or did not survive the Titanic disaster. Why would it be misleading for us to have plotted these as *frequency* histograms instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would have been misleading because frequency histograms would just show the total amount of surivors / non-surivors in total for each bin rather than showing the relative ratio between all age groups. We are more interested in seeing the relationships between all age groups rather than each age group separataly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part G**: Do the data suggest class warfare, male chivalry, age bias, or some combination of these characteristics in the final hours aboard the Titanic?  Justify your conclusions based on the computations done above, or do any other analysis that you like, but be sure to clearly justify your conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data above, we know that male chivalry was a factor in the final hours. Then looking at the density histogram of the various age groups survival rate we see that the younger age groups under had high priority to get to a boat and also the age group from 50 and up. So we could infer that between 20 and 50, they had less priority to get to a boat first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P.S.** It is not a component of your graded assignment, but the 1997 James Cameron film _Titanic_ captured some of these very notions in some riveting cinema. Whether or not you found evidence for these cases in the data, you might find it interesting to watch the movie (or just the segments of the sinking) to see one interpretation of these ideas. You can perhaps see how we might be persuaded to reinterpret the evidence of data by a heart-wrenching performance from a handsome young Leonardo DiCaprio!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Appendix'></a>\n",
    "\n",
    "## Appendix \n",
    "\n",
    "*Goal*: Prove that \n",
    "$$\n",
    "\\bar{x}_n = \\bar{x}_{n-1} + \\frac{x_n - \\bar{x}_{n-1}}{n}\n",
    "$$\n",
    "\n",
    "Note that you can get an expression for $\\bar{x}_{n-1}$ by simply replacing $n$ in Equation 1 above with $n-1$.\n",
    "\n",
    "We'll start with $\\bar{x}_n$ and massage it until we get the righthand side of the formula\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\nonumber \\bar{x}_n &=& \\frac{1}{n} \\sum_{k=1}^n x_k \\\\\n",
    "&=& \\frac{1}{n} \\sum_{k=1}^{n-1} x_k + \\frac{1}{n}x_n \\\\\n",
    "&=& \\frac{n-1}{n-1}\\frac{1}{n} \\sum_{k=1}^{n-1} x_k + \\frac{1}{n}x_n \\\\\n",
    "&=& \\frac{n-1}{n} \\left(\\frac{1}{n-1} \\sum_{k=1}^{n-1} x_k\\right) + \\frac{1}{n}x_n \\\\\n",
    "&=& \\frac{n-1}{n} \\bar{x}_{n-1} + \\frac{1}{n}x_n \\\\\n",
    "&=& \\frac{n}{n}\\bar{x}_{n-1} - \\frac{1}{n}\\bar{x}_{n-1} + \\frac{1}{n}x_n \\\\\n",
    "&=&  \\bar{x}_{n-1} + \\frac{x_n - \\bar{x}_{n-1}}{n} \\quad \\checkmark\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the track with shortest compute time\n",
    "init = 53\n",
    "proc = [98,183,37,122,14,124,65,67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# SSTF \n",
    "# find the minimum track from the current head, then compute the track distance \n",
    "# function will determine the distance and also record the order \n",
    "\n",
    "init = 53\n",
    "proc1 = [98,183,37,122,14,124,65,67]\n",
    "orders = []\n",
    "distance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init2 = 2150\n",
    "proc2 = [2069,1212,2296,2800,544,1618,356,1523,4965,3681]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "C-SCAN algorithm\n",
    "'''\n",
    "LOWER_CYLINDER = 0\n",
    "UPPER_CYLINDER = 4999\n",
    "\n",
    "def C_SCAN(requests, initialPosition):\n",
    "    localRequests = list(requests)\n",
    "    position = initialPosition\n",
    "    movement = 0\n",
    " \n",
    "    while localRequests:\n",
    "        if position in localRequests:\n",
    "            print(\"Servicing \" + str(position))\n",
    "            localRequests.remove(position)\n",
    " \n",
    "            if not localRequests:\n",
    "                break\n",
    "     \n",
    "        movement += 1\n",
    "        position += 1\n",
    "        if position == UPPER_CYLINDER:\n",
    "            position = 0\n",
    "            movement += UPPER_CYLINDER\n",
    " \n",
    "    return movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init2 = 2150\n",
    "proc2 = [2069,1212,2296,2800,544,1618,356,1523,4965,3681]\n",
    "C_SCAN(proc2,init2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSTF disk scheduling    \n",
    "def calculateDifference(queue, head, diff): \n",
    "    for i in range(len(diff)): \n",
    "        diff[i][0] = abs(queue[i] - head)  \n",
    "      \n",
    "# find unaccessed track which is  \n",
    "# at minimum distance from head  \n",
    "def findMin(diff):  \n",
    "  \n",
    "    index = -1\n",
    "    minimum = 999999999\n",
    "  \n",
    "    for i in range(len(diff)): \n",
    "        if (not diff[i][1] and \n",
    "                minimum > diff[i][0]): \n",
    "            minimum = diff[i][0] \n",
    "            index = i \n",
    "    return index  \n",
    "      \n",
    "def shortestSeekTimeFirst(request, head):              \n",
    "        if (len(request) == 0):  \n",
    "            return\n",
    "          \n",
    "        l = len(request)  \n",
    "        diff = [0] * l \n",
    "          \n",
    "        # initialize array  \n",
    "        for i in range(l): \n",
    "            diff[i] = [0, 0] \n",
    "          \n",
    "        # count total number of seek operation      \n",
    "        seek_count = 0\n",
    "          \n",
    "        # stores sequence in which disk  \n",
    "        # access is done  \n",
    "        seek_sequence = [0] * (l + 1)  \n",
    "          \n",
    "        for i in range(l):  \n",
    "            seek_sequence[i] = head  \n",
    "            calculateDifference(request, head, diff)  \n",
    "            index = findMin(diff)  \n",
    "              \n",
    "            diff[index][1] = True\n",
    "              \n",
    "            # increase the total count  \n",
    "            seek_count += diff[index][0]  \n",
    "              \n",
    "            # accessed track is now new head  \n",
    "            head = request[index]  \n",
    "      \n",
    "        # for last accessed track  \n",
    "        seek_sequence[len(seek_sequence) - 1] = head  \n",
    "          \n",
    "        print(\"Total number of seek operations =\",  \n",
    "                                       seek_count)                \n",
    "        print(\"Seek Sequence is\")  \n",
    "          \n",
    "        # print the sequence  \n",
    "        for i in range(l + 1): \n",
    "            print(seek_sequence[i])  \n",
    "shortestSeekTimeFirst(proc2, init2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "disk_size = 5000\n",
    " \n",
    "def SCAN(arr, head, direction):\n",
    " \n",
    "    seek_count = 0\n",
    "    distance, cur_track = 0, 0\n",
    "    left = []\n",
    "    right = []\n",
    "    seek_sequence = []\n",
    " \n",
    "    # Appending end values\n",
    "    # which has to be visited\n",
    "    # before reversing the direction\n",
    "    if (direction == \"left\"):\n",
    "        left.append(0)\n",
    "    elif (direction == \"right\"):\n",
    "        right.append(disk_size - 1)\n",
    " \n",
    "    for i in range(size):\n",
    "        if (arr[i] < head):\n",
    "            left.append(arr[i])\n",
    "        if (arr[i] > head):\n",
    "            right.append(arr[i])\n",
    " \n",
    "    # Sorting left and right vectors\n",
    "    left.sort()\n",
    "    right.sort()\n",
    " \n",
    "    # Run the while loop two times.\n",
    "    # one by one scanning right\n",
    "    # and left of the head\n",
    "    run = 2\n",
    "    while (run != 0):\n",
    "        if (direction == \"left\"):\n",
    "            for i in range(len(left) - 1, -1, -1):\n",
    "                cur_track = left[i]\n",
    " \n",
    "                # Appending current track to \n",
    "                # seek sequence\n",
    "                seek_sequence.append(cur_track)\n",
    " \n",
    "                # Calculate absolute distance\n",
    "                distance = abs(cur_track - head)\n",
    " \n",
    "                # Increase the total count\n",
    "                seek_count += distance\n",
    " \n",
    "                # Accessed track is now the new head\n",
    "                head = cur_track\n",
    "             \n",
    "            direction = \"right\"\n",
    "     \n",
    "        elif (direction == \"right\"):\n",
    "            for i in range(len(right)):\n",
    "                cur_track = right[i]\n",
    "                 \n",
    "                # Appending current track to seek \n",
    "                # sequence\n",
    "                seek_sequence.append(cur_track)\n",
    " \n",
    "                # Calculate absolute distance\n",
    "                distance = abs(cur_track - head)\n",
    " \n",
    "                # Increase the total count\n",
    "                seek_count += distance\n",
    " \n",
    "                # Accessed track is now new head\n",
    "                head = cur_track\n",
    "             \n",
    "            direction = \"left\"\n",
    "         \n",
    "        run -= 1\n",
    " \n",
    "    print(\"Total number of seek operations =\", \n",
    "          seek_count)\n",
    " \n",
    "    print(\"Seek Sequence is\")\n",
    " \n",
    "    for i in range(len(seek_sequence)):\n",
    "        print(seek_sequence[i])\n",
    " \n",
    "# Driver code\n",
    " \n",
    "# request array\n",
    "init3 = 2150\n",
    "proc3 = [2069,1212,2296,2800,544,1618,356,1523,4965,3681]\n",
    "direction = \"right\" # assuming 0 is the inner part of the disk\n",
    " \n",
    "SCAN(proc3, init3, direction)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from heapq import *\n",
    "# hp is initial head position\n",
    "# and requests is the list of requests\n",
    "# no of cylinders is 200\n",
    "def FCFS(hp,requests):\n",
    "\ttime = 0\n",
    "\tn = len(requests)\n",
    "\tpos = hp\n",
    "\tfor request in requests:\n",
    "\t\ttime += abs(request-pos)\n",
    "\t\tpos = request\n",
    "\t\tprint(\"        \",pos,\"  seeked\")\n",
    "\n",
    "\t# calculate average seek time\n",
    "\tavg_seek_time = time / n\n",
    "\treturn avg_seek_time\n",
    "\n",
    "# Shortest Seek Time First\n",
    "def SSTF(hp,reqs):\n",
    "\trequests = reqs.copy()\n",
    "\ttime = 0\n",
    "\tposition = hp\n",
    "\tn = len(requests)\n",
    "\theap=[]\n",
    "\twhile len(requests)>0:\n",
    "\t\tfor r in requests:\n",
    "\t\t\theappush(heap,(abs(position-r),r))\n",
    "\t\tx=heappop(heap)[1]\n",
    "\t\ttime+=abs(position-x)\n",
    "\t\tposition=x\n",
    "\t\tprint(\"        \",x,\"  seeked\")\n",
    "\t\trequests.remove(x)\n",
    "\t\theap=[]\n",
    "\n",
    "\t# calculate average seek time\n",
    "\tavg_seek_time = time/n\n",
    "\treturn avg_seek_time\n",
    "\n",
    "def C_SCAN(hp,reqs):\n",
    "\trequests = reqs.copy()\n",
    "\tpos = hp\n",
    "\ttime = 0\n",
    "\tend=5000\n",
    "\tstart=0\n",
    "\t#seek from curr_pos to end which is 200\n",
    "\tfor i in range(pos,end+1):\n",
    "\t\tif i in requests:\n",
    "\t\t\ttime+=abs(pos-i)\n",
    "\t\t\tpos=i\n",
    "\t\t\tprint(\"        \",i,\"  seeked\")\n",
    "\t\t\trequests.remove(i)\n",
    "\ttime+=abs(pos-end)\n",
    "\tpos=end\n",
    "\t#seek to hp from start\n",
    "\tfor i in range(start,hp+1):\n",
    "\t\tif i in requests:\n",
    "\t\t\ttime+=abs(pos-i)\n",
    "\t\t\tpos=i\n",
    "\t\t\tprint(\"        \",i,\"  seeked\")\n",
    "\t\t\trequests.remove(i)\n",
    "\t\n",
    "\t# calculate average seek time\n",
    "\tavg_seek_time = time/n\n",
    "\treturn avg_seek_time\n",
    "\n",
    "def LOOK(hp,reqs):\n",
    "\trequests = reqs.copy()\n",
    "\tpos = hp\n",
    "\ttime = 0\n",
    "\tend=max(requests)\n",
    "\tstart=min(requests)\n",
    "\t#seek from curr_pos to end which is 200\n",
    "\tfor i in range(pos,end+1):\n",
    "\t\tif i in requests:\n",
    "\t\t\ttime+=abs(pos-i)\n",
    "\t\t\tpos=i\n",
    "\t\t\tprint(\"        \",i,\"  seeked\")\n",
    "\t\t\trequests.remove(i)\n",
    "\n",
    "\t#seek back to start\n",
    "\tfor i in range(end,start-1,-1):\n",
    "\t\tif i in requests:\n",
    "\t\t\ttime+=abs(pos-i)\n",
    "\t\t\tpos=i\n",
    "\t\t\tprint(\"        \",i,\"  seeked\")\n",
    "\t\t\trequests.remove(i)\n",
    "\tprint(time)\n",
    "\t# calculate average seek time\n",
    "\tavg_seek_time = time/n\n",
    "\treturn avg_seek_time\n",
    "\n",
    "def C_LOOK(hp,reqs):\n",
    "\trequests = reqs.copy()\n",
    "\tpos = hp\n",
    "\ttime = 0\n",
    "\tend=max(requests)\n",
    "\tstart=min(requests)\n",
    "\t#seek from curr_pos to max of list \n",
    "\tfor i in range(pos,end+1):\n",
    "\t\tif i in requests:\n",
    "\t\t\ttime+=abs(pos-i)\n",
    "\t\t\tpos=i\n",
    "\t\t\tprint(\"        \",i,\"  seeked\")\n",
    "\t\t\trequests.remove(i)\n",
    "\n",
    "\ttime+=abs(pos-start)\n",
    "\tpos=start\n",
    "\t#seek to hp from start\n",
    "\tfor i in range(start,hp+1):\n",
    "\t\tif i in requests:\n",
    "\t\t\ttime+=abs(pos-i)\n",
    "\t\t\tpos=i\n",
    "\t\t\tprint(\"        \",i,\"  seeked\")\n",
    "\t\t\trequests.remove(i)\n",
    "\t\n",
    "\t# calculate average seek time\n",
    "\tavg_seek_time = time/n\n",
    "\treturn avg_seek_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init2 = 2150\n",
    "proc2 = [2069,1212,2296,2800,544,1618,356,1523,4965,3681]\n",
    "C_LOOK(init2, proc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init3 = 150\n",
    "proc3 = [32,901,447,432,637,506,851,679,173,313]\n",
    "SSTF(init3,proc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAGE REPLACEMENT ALGORITHMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIFO: best you can do is check you got the correct page fault count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1,2,3,4,2,1,5,6,7,1]\n",
    "n = len(a)\n",
    "m = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total page faults:  10\n"
     ]
    }
   ],
   "source": [
    "#First In First Out Page Replacement Algorithm\n",
    "def fifo():\n",
    "    global a,n,m\n",
    "    f = -1\n",
    "    page_faults = 0\n",
    "    page = []\n",
    "    for i in range(m):\n",
    "        page.append(-1)\n",
    "\n",
    "    for i in range(n):\n",
    "        flag = 0\n",
    "        for j in range(m):\n",
    "            if(page[j] == a[i]):\n",
    "                flag = 1\n",
    "                break\n",
    "\n",
    "        if flag == 0:\n",
    "            f=(f+1)%m\n",
    "            page[f] = a[i]\n",
    "            page_faults+=1\n",
    "            for j in range(m):\n",
    "                if page[j] != -1:\n",
    "                    x = 1\n",
    "    print(\"Total page faults: \", page_faults)\n",
    "    \n",
    "fifo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimal Page Replacement Algorithm\n",
    "def optimal():\n",
    "    global a,n,m\n",
    "    x = 0\n",
    "    page_faults = 0\n",
    "    page = []\n",
    "    for i in range(m):\n",
    "        page.append(-1)\n",
    "\n",
    "    for i in range(n):\n",
    "        flag = 0\n",
    "        for j in range(m):\n",
    "            if(page[j] == a[i]):\n",
    "                flag = 1\n",
    "                break\n",
    "            \n",
    "        if flag == 0:\n",
    "            if page[x] != -1:\n",
    "                max = -1\n",
    "                for k in range(m):\n",
    "                    flag = 0\n",
    "                    j =  i\n",
    "                    while j<n:\n",
    "                        j+=1\n",
    "                        if(page[k] == a[j]):\n",
    "                            flag = 1\n",
    "                            break\n",
    "                    if (flag == 1 and min < j):\n",
    "                        max = j\n",
    "                        x = k\n",
    "\n",
    "            page[x] = a[i]\n",
    "            x=(x+1)%m\n",
    "            page_faults+=1\n",
    "            print \"\\n%d ->\" % (a[i]),\n",
    "            for j in range(m):\n",
    "                if page[j] != -1:\n",
    "                    print page[j],\n",
    "                else:\n",
    "                    print \"-\",\n",
    "        else:\n",
    "            print \"\\n%d -> No Page Fault\" % (a[i]),\n",
    "            \n",
    "    print \"\\n Total page faults : %d.\" % (page_faults)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
